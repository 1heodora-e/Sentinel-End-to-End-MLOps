{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sentinel Model - Audio Distress Detection\n",
        "\n",
        "Model architecture and training logic using MobileNetV2 for transfer learning on audio spectrograms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import json\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root and backend to Python path for imports\n",
        "project_root = Path().resolve().parent\n",
        "backend_path = project_root / \"backend\"\n",
        "sys.path.insert(0, str(project_root))\n",
        "sys.path.insert(0, str(backend_path))\n",
        "\n",
        "# Enable eager execution (required for TensorFlow/Keras 2.x+)\n",
        "tf.config.run_functions_eagerly(True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "# Determine model directory based on where we're running from\n",
        "_script_dir = Path(\".\")\n",
        "if Path(\".\").absolute().name == \"backend\":\n",
        "    MODEL_DIR = Path(\"models\")\n",
        "else:\n",
        "    MODEL_DIR = Path(\"backend/models\")\n",
        "\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "MODEL_PATH = MODEL_DIR / \"sentinel_model.h5\"\n",
        "METADATA_PATH = MODEL_DIR / \"model_metadata.json\"\n",
        "\n",
        "# Image input shape (MobileNetV2 default)\n",
        "INPUT_SHAPE = (224, 224, 3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Architecture\n",
        "\n",
        "Create MobileNetV2-based model for binary audio classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_model(input_shape=INPUT_SHAPE, num_classes=2, weights=None):\n",
        "    \"\"\"\n",
        "    Create MobileNetV2-based model for binary audio classification.\n",
        "    \n",
        "    Args:\n",
        "        input_shape: Input image shape (default: (224, 224, 3))\n",
        "        num_classes: Number of output classes (default: 2 for binary)\n",
        "        weights: Path to pretrained weights or None for random init\n",
        "    \n",
        "    Returns:\n",
        "        Compiled Keras model\n",
        "    \"\"\"\n",
        "    # Base MobileNetV2 (pretrained on ImageNet, excluding top)\n",
        "    base_model = MobileNetV2(\n",
        "        input_shape=input_shape,\n",
        "        include_top=False,\n",
        "        weights='imagenet' if weights is None else None,\n",
        "        alpha=1.0\n",
        "    )\n",
        "    \n",
        "    # Freeze base model initially (can be unfrozen during fine-tuning)\n",
        "    base_model.trainable = False\n",
        "    \n",
        "    # Build custom classifier head\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    \n",
        "    # Preprocess for MobileNetV2\n",
        "    x = base_model(inputs, training=False)\n",
        "    \n",
        "    # Global average pooling\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    \n",
        "    # Dense layers\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    \n",
        "    # Output layer (binary classification)\n",
        "    if num_classes == 2:\n",
        "        outputs = layers.Dense(1, activation='sigmoid', name='predictions')(x)\n",
        "        loss = 'binary_crossentropy'\n",
        "    else:\n",
        "        outputs = layers.Dense(num_classes, activation='softmax', name='predictions')(x)\n",
        "        loss = 'sparse_categorical_crossentropy'\n",
        "    \n",
        "    model = keras.Model(inputs, outputs, name='sentinel_mobilenet')\n",
        "    \n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss=loss,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model(model_path=MODEL_PATH):\n",
        "    \"\"\"\n",
        "    Load trained model from disk.\n",
        "    \n",
        "    Args:\n",
        "        model_path: Path to saved model file\n",
        "    \n",
        "    Returns:\n",
        "        Loaded Keras model, or None if file doesn't exist\n",
        "    \"\"\"\n",
        "    if os.path.exists(model_path):\n",
        "        try:\n",
        "            model = keras.models.load_model(model_path)\n",
        "            return model\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {e}\")\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "\n",
        "def save_model(model, model_path=MODEL_PATH, metadata=None):\n",
        "    \"\"\"\n",
        "    Save model and optional metadata to disk.\n",
        "    \n",
        "    Args:\n",
        "        model: Keras model to save\n",
        "        model_path: Path to save model\n",
        "        metadata: Optional dictionary of metadata to save\n",
        "    \"\"\"\n",
        "    model.save(model_path)\n",
        "    \n",
        "    if metadata:\n",
        "        with open(METADATA_PATH, 'w') as f:\n",
        "            json.dump(metadata, f, indent=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _prepare_data_from_directories(data_dir, validation_split=0.2):\n",
        "    \"\"\"\n",
        "    Internal function: Prepare training data from directory structure:\n",
        "    data_dir/\n",
        "        safe/\n",
        "            audio1.wav\n",
        "            audio2.wav\n",
        "        danger/\n",
        "            audio1.wav\n",
        "            audio2.wav\n",
        "    \n",
        "    Args:\n",
        "        data_dir: Root directory containing class subdirectories\n",
        "        validation_split: Fraction of data to use for validation\n",
        "    \n",
        "    Returns:\n",
        "        train_generator, val_generator, num_samples\n",
        "    \"\"\"\n",
        "    from backend.preprocessing import audio_file_to_image, image_to_array\n",
        "    \n",
        "    data_path = Path(data_dir)\n",
        "    safe_dir = data_path / \"safe\"\n",
        "    danger_dir = data_path / \"danger\"\n",
        "    \n",
        "    # Collect all audio files\n",
        "    safe_files = list(Path(safe_dir).glob(\"*.wav\")) if safe_dir.exists() else []\n",
        "    danger_files = list(Path(danger_dir).glob(\"*.wav\")) if danger_dir.exists() else []\n",
        "    \n",
        "    if len(safe_files) == 0 and len(danger_files) == 0:\n",
        "        raise ValueError(f\"No audio files found in {data_dir}\")\n",
        "    \n",
        "    # Process audio files to images\n",
        "    X = []\n",
        "    y = []\n",
        "    \n",
        "    for file_path in safe_files:\n",
        "        try:\n",
        "            img = audio_file_to_image(file_path)\n",
        "            img_array = image_to_array(img)\n",
        "            X.append(img_array)\n",
        "            y.append(0)  # Safe class\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {file_path}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    for file_path in danger_files:\n",
        "        try:\n",
        "            img = audio_file_to_image(file_path)\n",
        "            img_array = image_to_array(img)\n",
        "            X.append(img_array)\n",
        "            y.append(1)  # Danger class\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {file_path}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    if len(X) == 0:\n",
        "        raise ValueError(\"No valid audio files could be processed\")\n",
        "    \n",
        "    # Convert to numpy arrays (ensure they're proper numpy arrays, not tensorflow tensors)\n",
        "    X = np.array(X, dtype=np.float32)\n",
        "    y = np.array(y, dtype=np.float32)\n",
        "    \n",
        "    # Shuffle data\n",
        "    indices = np.random.permutation(len(X))\n",
        "    X = X[indices]\n",
        "    y = y[indices]\n",
        "    \n",
        "    # Split into train/validation\n",
        "    split_idx = int(len(X) * (1 - validation_split))\n",
        "    X_train, X_val = X[:split_idx], X[split_idx:]\n",
        "    y_train, y_val = y[:split_idx], y[split_idx:]\n",
        "    \n",
        "    # Apply data augmentation\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=5,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        zoom_range=0.1,\n",
        "        horizontal_flip=False,  # Don't flip spectrograms\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "    \n",
        "    train_generator = datagen.flow(X_train, y_train, batch_size=32, shuffle=True)\n",
        "    val_generator = datagen.flow(X_val, y_val, batch_size=32, shuffle=False)\n",
        "    \n",
        "    num_samples = len(X)\n",
        "    \n",
        "    return train_generator, val_generator, num_samples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(data_dir, epochs=10, batch_size=32, validation_split=0.2, \n",
        "                initial_epoch=0, model=None):\n",
        "    \"\"\"\n",
        "    Train the Sentinel model on audio data.\n",
        "    \n",
        "    Args:\n",
        "        data_dir: Directory containing safe/ and danger/ subdirectories\n",
        "        epochs: Number of training epochs\n",
        "        batch_size: Batch size for training\n",
        "        validation_split: Fraction of data for validation\n",
        "        initial_epoch: Starting epoch (for resuming training)\n",
        "        model: Existing model to continue training, or None to create new\n",
        "    \n",
        "    Returns:\n",
        "        Trained model and training history\n",
        "    \"\"\"\n",
        "    # Load or create model\n",
        "    if model is None:\n",
        "        model = load_model()\n",
        "        if model is None:\n",
        "            print(\"Creating new model...\")\n",
        "            model = create_model()\n",
        "    \n",
        "    # Prepare data\n",
        "    print(f\"Loading data from {data_dir}...\")\n",
        "    train_gen, val_gen, num_samples = _prepare_data_from_directories(\n",
        "        data_dir, validation_split=validation_split\n",
        "    )\n",
        "    \n",
        "    print(f\"Training on {num_samples} samples...\")\n",
        "    \n",
        "    # Callbacks\n",
        "    callbacks = [\n",
        "        keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=5,\n",
        "            restore_best_weights=True\n",
        "        ),\n",
        "        keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=3,\n",
        "            min_lr=1e-7\n",
        "        )\n",
        "    ]\n",
        "    \n",
        "    # Train model\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        epochs=epochs,\n",
        "        validation_data=val_gen,\n",
        "        callbacks=callbacks,\n",
        "        initial_epoch=initial_epoch,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    # Save model\n",
        "    metadata = {\n",
        "        'epochs_trained': epochs,\n",
        "        'total_samples': num_samples,\n",
        "        'last_accuracy': float(history.history['accuracy'][-1]),\n",
        "        'last_val_accuracy': float(history.history['val_accuracy'][-1])\n",
        "    }\n",
        "    \n",
        "    save_model(model, metadata=metadata)\n",
        "    \n",
        "    print(\"Model training completed and saved!\")\n",
        "    \n",
        "    return model, history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(model, audio_path):\n",
        "    \"\"\"\n",
        "    Make prediction on a single audio file.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained Keras model\n",
        "        audio_path: Path to audio file or file-like object\n",
        "    \n",
        "    Returns:\n",
        "        dict with 'prediction' (0 or 1), 'confidence' (float), 'probability' (float)\n",
        "    \"\"\"\n",
        "    from backend.preprocessing import audio_file_to_image, image_to_array\n",
        "    \n",
        "    # Process audio\n",
        "    img = audio_file_to_image(audio_path)\n",
        "    img_array = image_to_array(img)\n",
        "    \n",
        "    # Add batch dimension\n",
        "    img_batch = np.expand_dims(img_array, axis=0)\n",
        "    \n",
        "    # Predict\n",
        "    prediction = model.predict(img_batch, verbose=0)[0][0]\n",
        "    \n",
        "    # Binary classification: 0 = Safe, 1 = Danger\n",
        "    class_idx = 1 if prediction > 0.5 else 0\n",
        "    confidence = abs(prediction - 0.5) * 2  # Convert to [0, 1] confidence\n",
        "    \n",
        "    return {\n",
        "        'prediction': int(class_idx),\n",
        "        'class': 'danger' if class_idx == 1 else 'safe',\n",
        "        'confidence': float(confidence),\n",
        "        'probability': float(prediction)  # Raw probability (0=Safe, 1=Danger)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interactive Training\n",
        "\n",
        "Now let's actually train the model with your data!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data directory: c:\\Users\\USER\\Documents\\Sentinel-End-to-End-MLOps\\notebook\\..\\backend\\data\n",
            "Safe directory exists: True\n",
            "Danger directory exists: True\n",
            "Safe audio files: 100\n",
            "Danger audio files: 100\n"
          ]
        }
      ],
      "source": [
        "# Check data directory structure (look in backend/data)\n",
        "from pathlib import Path\n",
        "\n",
        "# Try backend/data first, then data\n",
        "data_dir = Path(\"../backend/data\") if Path(\"../backend/data\").exists() else Path(\"data\")\n",
        "safe_dir = data_dir / \"safe\"\n",
        "danger_dir = data_dir / \"danger\"\n",
        "\n",
        "print(f\"Data directory: {data_dir.absolute()}\")\n",
        "print(f\"Safe directory exists: {safe_dir.exists()}\")\n",
        "print(f\"Danger directory exists: {danger_dir.exists()}\")\n",
        "\n",
        "if safe_dir.exists():\n",
        "    safe_files = list(safe_dir.glob(\"*.wav\"))\n",
        "    print(f\"Safe audio files: {len(safe_files)}\")\n",
        "    \n",
        "if danger_dir.exists():\n",
        "    danger_files = list(danger_dir.glob(\"*.wav\"))\n",
        "    print(f\"Danger audio files: {len(danger_files)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing model...\n",
            "Creating new model...\n",
            "New model created!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sentinel_mobilenet\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sentinel_mobilenet\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">163,968</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ predictions (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m163,968\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ predictions (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,430,273</span> (9.27 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,430,273\u001b[0m (9.27 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">172,289</span> (673.00 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m172,289\u001b[0m (673.00 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create or load model\n",
        "print(\"Initializing model...\")\n",
        "model = load_model()\n",
        "if model is None:\n",
        "    print(\"Creating new model...\")\n",
        "    model = create_model()\n",
        "    print(\"New model created!\")\n",
        "else:\n",
        "    print(\"Loaded existing model from disk\")\n",
        "    \n",
        "# Display model summary\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the Model\n",
        "\n",
        "Train the model on your preprocessed data. Adjust epochs, batch_size, and validation_split as needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Starting Model Training\n",
            "============================================================\n",
            "Epochs: 10\n",
            "Batch size: 32\n",
            "Validation split: 0.2\n",
            "Data directory: ../backend/data\n",
            "============================================================\n",
            "Loading data from ../backend/data...\n",
            "Training on 200 samples...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USER\\Documents\\Sentinel-End-to-End-MLOps\\venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 4s/step - accuracy: 0.4625 - loss: 0.9610 - val_accuracy: 0.7250 - val_loss: 0.6035 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4s/step - accuracy: 0.6438 - loss: 0.6331 - val_accuracy: 0.7000 - val_loss: 0.6121 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4s/step - accuracy: 0.6625 - loss: 0.6220 - val_accuracy: 0.6500 - val_loss: 0.5422 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4s/step - accuracy: 0.6812 - loss: 0.6248 - val_accuracy: 0.8000 - val_loss: 0.5324 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4s/step - accuracy: 0.7188 - loss: 0.5914 - val_accuracy: 0.7750 - val_loss: 0.5284 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4s/step - accuracy: 0.7188 - loss: 0.6024 - val_accuracy: 0.8750 - val_loss: 0.4810 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4s/step - accuracy: 0.7437 - loss: 0.5265 - val_accuracy: 0.8500 - val_loss: 0.4497 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4s/step - accuracy: 0.7875 - loss: 0.4639 - val_accuracy: 0.8000 - val_loss: 0.4660 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 5s/step - accuracy: 0.8062 - loss: 0.4219 - val_accuracy: 0.8000 - val_loss: 0.4615 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5s/step - accuracy: 0.7688 - loss: 0.4649 - val_accuracy: 0.8250 - val_loss: 0.4167 - learning_rate: 0.0010\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model training completed and saved!\n",
            "\n",
            "Training completed!\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "# Adjust these parameters as needed:\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 32\n",
        "VALIDATION_SPLIT = 0.2\n",
        "# Use backend/data if it exists, otherwise try data\n",
        "DATA_DIR = \"../backend/data\" if Path(\"../backend/data\").exists() else \"data\"\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Starting Model Training\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Epochs: {EPOCHS}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Validation split: {VALIDATION_SPLIT}\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Train the model\n",
        "trained_model, training_history = train_model(\n",
        "    data_dir=DATA_DIR,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_split=VALIDATION_SPLIT,\n",
        "    model=model  # Use the model we created/loaded above\n",
        ")\n",
        "\n",
        "print(\"\\nTraining completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Training Results\n",
        "\n",
        "Plot training accuracy and loss over epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final Training Accuracy: 0.7688\n",
            "Final Validation Accuracy: 0.8250\n",
            "Final Training Loss: 0.4649\n",
            "Final Validation Loss: 0.4167\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_22704\\3664975263.py:34: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
            "  plt.show()\n"
          ]
        }
      ],
      "source": [
        "# Plot training history\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check if training_history exists (may not if training failed)\n",
        "if 'training_history' in globals():\n",
        "    history = training_history.history\n",
        "else:\n",
        "    print(\"⚠️ Error: training_history not found. Please run the training cell first.\")\n",
        "    print(\"   If training failed, check the error messages above.\")\n",
        "    raise NameError(\"training_history is not defined. Run the training cell first.\")\n",
        "\n",
        "# Create figure with subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot accuracy\n",
        "ax1.plot(history['accuracy'], label='Training Accuracy', marker='o')\n",
        "ax1.plot(history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.set_title('Model Accuracy')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "# Plot loss\n",
        "ax2.plot(history['loss'], label='Training Loss', marker='o')\n",
        "ax2.plot(history['val_loss'], label='Validation Loss', marker='s')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.set_title('Model Loss')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print final metrics\n",
        "print(f\"\\nFinal Training Accuracy: {history['accuracy'][-1]:.4f}\")\n",
        "print(f\"Final Validation Accuracy: {history['val_accuracy'][-1]:.4f}\")\n",
        "print(f\"Final Training Loss: {history['loss'][-1]:.4f}\")\n",
        "print(f\"Final Validation Loss: {history['val_loss'][-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Predictions\n",
        "\n",
        "Test the trained model on sample audio files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing on SAFE file: xlE_HYdFR-I_out.wav\n",
            "Prediction: SAFE\n",
            "Confidence: 48.31%\n",
            "Probability: 0.2585\n",
            "Expected: SAFE | Got: SAFE | ✓\n",
            "\n",
            "Testing on DANGER file: rMAvJseEE0I_out.wav\n",
            "Prediction: DANGER\n",
            "Confidence: 70.72%\n",
            "Probability: 0.8536\n",
            "Expected: DANGER | Got: DANGER | ✓\n"
          ]
        }
      ],
      "source": [
        "# Test prediction on a sample file\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "# Get sample files\n",
        "data_dir = Path(\"../backend/data\") if Path(\"../backend/data\").exists() else Path(\"data\")\n",
        "safe_files = list((data_dir / \"safe\").glob(\"*.wav\")) if (data_dir / \"safe\").exists() else []\n",
        "danger_files = list((data_dir / \"danger\").glob(\"*.wav\")) if (data_dir / \"danger\").exists() else []\n",
        "\n",
        "# Test on a random safe file\n",
        "if safe_files:\n",
        "    test_safe = random.choice(safe_files)\n",
        "    print(f\"Testing on SAFE file: {test_safe.name}\")\n",
        "    result = predict(trained_model, test_safe)\n",
        "    print(f\"Prediction: {result['class'].upper()}\")\n",
        "    print(f\"Confidence: {result['confidence']:.2%}\")\n",
        "    print(f\"Probability: {result['probability']:.4f}\")\n",
        "    print(f\"Expected: SAFE | Got: {result['class'].upper()} | {'✓' if result['class'] == 'safe' else '✗'}\")\n",
        "\n",
        "# Test on a random danger file\n",
        "if danger_files:\n",
        "    test_danger = random.choice(danger_files)\n",
        "    print(f\"\\nTesting on DANGER file: {test_danger.name}\")\n",
        "    result = predict(trained_model, test_danger)\n",
        "    print(f\"Prediction: {result['class'].upper()}\")\n",
        "    print(f\"Confidence: {result['confidence']:.2%}\")\n",
        "    print(f\"Probability: {result['probability']:.4f}\")\n",
        "    print(f\"Expected: DANGER | Got: {result['class'].upper()} | {'✓' if result['class'] == 'danger' else '✗'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠️ Error: Please run the Model Evaluation cell first to define y_test and predictions variables.\n"
          ]
        }
      ],
      "source": [
        "## Advanced Model Evaluation Metrics (ROC and Precision-Recall Curves)\n",
        "\n",
        "# This cell should be run AFTER the Model Evaluation cell that defines y_test and predictions\n",
        "# Check if variables exist from previous cell\n",
        "if 'y_test' in globals() and 'predictions' in globals():\n",
        "    from sklearn.metrics import roc_curve, auc, roc_auc_score, precision_recall_curve, average_precision_score\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    # Flatten predictions for ROC curve (predictions is shape (n, 1))\n",
        "    predictions_flat = predictions.flatten() if len(predictions.shape) > 1 else predictions\n",
        "    \n",
        "    # Calculate ROC curve\n",
        "    fpr, tpr, _ = roc_curve(y_test, predictions_flat)\n",
        "    roc_auc = roc_auc_score(y_test, predictions_flat)\n",
        "    \n",
        "    # Plot ROC Curve\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate', fontsize=12)\n",
        "    plt.ylabel('True Positive Rate', fontsize=12)\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=14, fontweight='bold')\n",
        "    plt.legend(loc=\"lower right\", fontsize=11)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\nROC AUC Score: {roc_auc:.4f}\")\n",
        "    \n",
        "    # Precision-Recall Curve\n",
        "    precision, recall, _ = precision_recall_curve(y_test, predictions_flat)\n",
        "    avg_precision = average_precision_score(y_test, predictions_flat)\n",
        "    \n",
        "    # Plot Precision-Recall Curve\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(recall, precision, color='blue', lw=2, \n",
        "             label=f'Precision-Recall curve (AP = {avg_precision:.2f})')\n",
        "    plt.xlabel('Recall', fontsize=12)\n",
        "    plt.ylabel('Precision', fontsize=12)\n",
        "    plt.title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
        "    plt.legend(loc=\"lower left\", fontsize=11)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"Average Precision Score: {avg_precision:.4f}\")\n",
        "else:\n",
        "    print(\"⚠️ Error: Please run the Model Evaluation cell first to define y_test and predictions variables.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Evaluation\n",
        "\n",
        "Evaluate the model on all validation data to get comprehensive metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing test data...\n",
            "\n",
            "============================================================\n",
            "Classification Report\n",
            "============================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Safe       0.75      0.75      0.75        20\n",
            "      Danger       0.75      0.75      0.75        20\n",
            "\n",
            "    accuracy                           0.75        40\n",
            "   macro avg       0.75      0.75      0.75        40\n",
            "weighted avg       0.75      0.75      0.75        40\n",
            "\n",
            "\n",
            "Confusion Matrix\n",
            "============================================================\n",
            "[[15  5]\n",
            " [ 5 15]]\n",
            "\n",
            "True Negatives (Safe→Safe): 15\n",
            "False Positives (Safe→Danger): 5\n",
            "False Negatives (Danger→Safe): 5\n",
            "True Positives (Danger→Danger): 15\n"
          ]
        }
      ],
      "source": [
        "# Evaluate model on test data\n",
        "from backend.preprocessing import audio_file_to_image, image_to_array\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Prepare test data (you can adjust this to use a separate test set)\n",
        "test_data_dir = Path(\"../backend/data\") if Path(\"../backend/data\").exists() else Path(\"data\")\n",
        "safe_test = list((test_data_dir / \"safe\").glob(\"*.wav\"))[:20] if (test_data_dir / \"safe\").exists() else []  # Test on subset\n",
        "danger_test = list((test_data_dir / \"danger\").glob(\"*.wav\"))[:20] if (test_data_dir / \"danger\").exists() else []\n",
        "\n",
        "X_test = []\n",
        "y_test = []\n",
        "\n",
        "print(\"Preparing test data...\")\n",
        "for file in safe_test:\n",
        "    try:\n",
        "        img = audio_file_to_image(file)\n",
        "        img_array = image_to_array(img)\n",
        "        X_test.append(img_array)\n",
        "        y_test.append(0)  # Safe\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file}: {e}\")\n",
        "\n",
        "for file in danger_test:\n",
        "    try:\n",
        "        img = audio_file_to_image(file)\n",
        "        img_array = image_to_array(img)\n",
        "        X_test.append(img_array)\n",
        "        y_test.append(1)  # Danger\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file}: {e}\")\n",
        "\n",
        "if X_test:\n",
        "    X_test = np.array(X_test)\n",
        "    y_test = np.array(y_test)\n",
        "    \n",
        "    # Make predictions\n",
        "    predictions = trained_model.predict(X_test, verbose=0)\n",
        "    y_pred = (predictions > 0.5).astype(int).flatten()\n",
        "    \n",
        "    # Print metrics\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Classification Report\")\n",
        "    print(\"=\" * 60)\n",
        "    print(classification_report(y_test, y_pred, target_names=['Safe', 'Danger']))\n",
        "    \n",
        "    print(\"\\nConfusion Matrix\")\n",
        "    print(\"=\" * 60)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(cm)\n",
        "    print(\"\\nTrue Negatives (Safe→Safe):\", cm[0][0])\n",
        "    print(\"False Positives (Safe→Danger):\", cm[0][1])\n",
        "    print(\"False Negatives (Danger→Safe):\", cm[1][0])\n",
        "    print(\"True Positives (Danger→Danger):\", cm[1][1])\n",
        "    \n",
        "    # Store predictions for ROC curve analysis (flatten for compatibility)\n",
        "    predictions = predictions.flatten() if len(predictions.shape) > 1 else predictions\n",
        "else:\n",
        "    print(\"No test data available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save and Use Model\n",
        "\n",
        "The model is automatically saved during training. You can also manually save it here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to: backend\\models\\sentinel_model.h5\n",
            "Metadata saved to: backend\\models\\model_metadata.json\n",
            "\n",
            "Verifying model can be loaded...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Model loaded successfully!\n",
            "Model parameters: 2,430,273\n"
          ]
        }
      ],
      "source": [
        "# The model is already saved by train_model(), but you can save again with custom metadata\n",
        "metadata = {\n",
        "    'training_completed': True,\n",
        "    'epochs': EPOCHS,\n",
        "    'final_accuracy': float(training_history.history['accuracy'][-1]),\n",
        "    'final_val_accuracy': float(training_history.history['val_accuracy'][-1]),\n",
        "}\n",
        "\n",
        "save_model(trained_model, metadata=metadata)\n",
        "print(f\"Model saved to: {MODEL_PATH}\")\n",
        "print(f\"Metadata saved to: {METADATA_PATH}\")\n",
        "\n",
        "# Verify model can be loaded\n",
        "print(\"\\nVerifying model can be loaded...\")\n",
        "loaded_model = load_model()\n",
        "if loaded_model:\n",
        "    print(\"✓ Model loaded successfully!\")\n",
        "    print(f\"Model parameters: {loaded_model.count_params():,}\")\n",
        "else:\n",
        "    print(\"✗ Failed to load model\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
