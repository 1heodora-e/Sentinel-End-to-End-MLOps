{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sentinel Model - Audio Distress Detection\n",
        "\n",
        "Model architecture and training logic using MobileNetV2 for transfer learning on audio spectrograms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Enable eager execution (required for TensorFlow/Keras 2.x+)\n",
        "tf.config.run_functions_eagerly(True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "# Determine model directory based on where we're running from\n",
        "_script_dir = Path(\".\")\n",
        "if Path(\".\").absolute().name == \"backend\":\n",
        "    MODEL_DIR = Path(\"models\")\n",
        "else:\n",
        "    MODEL_DIR = Path(\"backend/models\")\n",
        "\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "MODEL_PATH = MODEL_DIR / \"sentinel_model.h5\"\n",
        "METADATA_PATH = MODEL_DIR / \"model_metadata.json\"\n",
        "\n",
        "# Image input shape (MobileNetV2 default)\n",
        "INPUT_SHAPE = (224, 224, 3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Architecture\n",
        "\n",
        "Create MobileNetV2-based model for binary audio classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_model(input_shape=INPUT_SHAPE, num_classes=2, weights=None):\n",
        "    \"\"\"\n",
        "    Create MobileNetV2-based model for binary audio classification.\n",
        "    \n",
        "    Args:\n",
        "        input_shape: Input image shape (default: (224, 224, 3))\n",
        "        num_classes: Number of output classes (default: 2 for binary)\n",
        "        weights: Path to pretrained weights or None for random init\n",
        "    \n",
        "    Returns:\n",
        "        Compiled Keras model\n",
        "    \"\"\"\n",
        "    # Base MobileNetV2 (pretrained on ImageNet, excluding top)\n",
        "    base_model = MobileNetV2(\n",
        "        input_shape=input_shape,\n",
        "        include_top=False,\n",
        "        weights='imagenet' if weights is None else None,\n",
        "        alpha=1.0\n",
        "    )\n",
        "    \n",
        "    # Freeze base model initially (can be unfrozen during fine-tuning)\n",
        "    base_model.trainable = False\n",
        "    \n",
        "    # Build custom classifier head\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    \n",
        "    # Preprocess for MobileNetV2\n",
        "    x = base_model(inputs, training=False)\n",
        "    \n",
        "    # Global average pooling\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    \n",
        "    # Dense layers\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    \n",
        "    # Output layer (binary classification)\n",
        "    if num_classes == 2:\n",
        "        outputs = layers.Dense(1, activation='sigmoid', name='predictions')(x)\n",
        "        loss = 'binary_crossentropy'\n",
        "    else:\n",
        "        outputs = layers.Dense(num_classes, activation='softmax', name='predictions')(x)\n",
        "        loss = 'sparse_categorical_crossentropy'\n",
        "    \n",
        "    model = keras.Model(inputs, outputs, name='sentinel_mobilenet')\n",
        "    \n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss=loss,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model(model_path=MODEL_PATH):\n",
        "    \"\"\"\n",
        "    Load trained model from disk.\n",
        "    \n",
        "    Args:\n",
        "        model_path: Path to saved model file\n",
        "    \n",
        "    Returns:\n",
        "        Loaded Keras model, or None if file doesn't exist\n",
        "    \"\"\"\n",
        "    if os.path.exists(model_path):\n",
        "        try:\n",
        "            model = keras.models.load_model(model_path)\n",
        "            return model\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {e}\")\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "\n",
        "def save_model(model, model_path=MODEL_PATH, metadata=None):\n",
        "    \"\"\"\n",
        "    Save model and optional metadata to disk.\n",
        "    \n",
        "    Args:\n",
        "        model: Keras model to save\n",
        "        model_path: Path to save model\n",
        "        metadata: Optional dictionary of metadata to save\n",
        "    \"\"\"\n",
        "    model.save(model_path)\n",
        "    \n",
        "    if metadata:\n",
        "        with open(METADATA_PATH, 'w') as f:\n",
        "            json.dump(metadata, f, indent=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _prepare_data_from_directories(data_dir, validation_split=0.2):\n",
        "    \"\"\"\n",
        "    Internal function: Prepare training data from directory structure:\n",
        "    data_dir/\n",
        "        safe/\n",
        "            audio1.wav\n",
        "            audio2.wav\n",
        "        danger/\n",
        "            audio1.wav\n",
        "            audio2.wav\n",
        "    \n",
        "    Args:\n",
        "        data_dir: Root directory containing class subdirectories\n",
        "        validation_split: Fraction of data to use for validation\n",
        "    \n",
        "    Returns:\n",
        "        train_generator, val_generator, num_samples\n",
        "    \"\"\"\n",
        "    from preprocessing import audio_file_to_image, image_to_array\n",
        "    \n",
        "    data_path = Path(data_dir)\n",
        "    safe_dir = data_path / \"safe\"\n",
        "    danger_dir = data_path / \"danger\"\n",
        "    \n",
        "    # Collect all audio files\n",
        "    safe_files = list(Path(safe_dir).glob(\"*.wav\")) if safe_dir.exists() else []\n",
        "    danger_files = list(Path(danger_dir).glob(\"*.wav\")) if danger_dir.exists() else []\n",
        "    \n",
        "    if len(safe_files) == 0 and len(danger_files) == 0:\n",
        "        raise ValueError(f\"No audio files found in {data_dir}\")\n",
        "    \n",
        "    # Process audio files to images\n",
        "    X = []\n",
        "    y = []\n",
        "    \n",
        "    for file_path in safe_files:\n",
        "        try:\n",
        "            img = audio_file_to_image(file_path)\n",
        "            img_array = image_to_array(img)\n",
        "            X.append(img_array)\n",
        "            y.append(0)  # Safe class\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {file_path}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    for file_path in danger_files:\n",
        "        try:\n",
        "            img = audio_file_to_image(file_path)\n",
        "            img_array = image_to_array(img)\n",
        "            X.append(img_array)\n",
        "            y.append(1)  # Danger class\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {file_path}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    if len(X) == 0:\n",
        "        raise ValueError(\"No valid audio files could be processed\")\n",
        "    \n",
        "    # Convert to numpy arrays (ensure they're proper numpy arrays, not tensorflow tensors)\n",
        "    X = np.array(X, dtype=np.float32)\n",
        "    y = np.array(y, dtype=np.float32)\n",
        "    \n",
        "    # Shuffle data\n",
        "    indices = np.random.permutation(len(X))\n",
        "    X = X[indices]\n",
        "    y = y[indices]\n",
        "    \n",
        "    # Split into train/validation\n",
        "    split_idx = int(len(X) * (1 - validation_split))\n",
        "    X_train, X_val = X[:split_idx], X[split_idx:]\n",
        "    y_train, y_val = y[:split_idx], y[split_idx:]\n",
        "    \n",
        "    # Apply data augmentation\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=5,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        zoom_range=0.1,\n",
        "        horizontal_flip=False,  # Don't flip spectrograms\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "    \n",
        "    train_generator = datagen.flow(X_train, y_train, batch_size=32, shuffle=True)\n",
        "    val_generator = datagen.flow(X_val, y_val, batch_size=32, shuffle=False)\n",
        "    \n",
        "    num_samples = len(X)\n",
        "    \n",
        "    return train_generator, val_generator, num_samples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(data_dir, epochs=10, batch_size=32, validation_split=0.2, \n",
        "                initial_epoch=0, model=None):\n",
        "    \"\"\"\n",
        "    Train the Sentinel model on audio data.\n",
        "    \n",
        "    Args:\n",
        "        data_dir: Directory containing safe/ and danger/ subdirectories\n",
        "        epochs: Number of training epochs\n",
        "        batch_size: Batch size for training\n",
        "        validation_split: Fraction of data for validation\n",
        "        initial_epoch: Starting epoch (for resuming training)\n",
        "        model: Existing model to continue training, or None to create new\n",
        "    \n",
        "    Returns:\n",
        "        Trained model and training history\n",
        "    \"\"\"\n",
        "    # Load or create model\n",
        "    if model is None:\n",
        "        model = load_model()\n",
        "        if model is None:\n",
        "            print(\"Creating new model...\")\n",
        "            model = create_model()\n",
        "    \n",
        "    # Prepare data\n",
        "    print(f\"Loading data from {data_dir}...\")\n",
        "    train_gen, val_gen, num_samples = _prepare_data_from_directories(\n",
        "        data_dir, validation_split=validation_split\n",
        "    )\n",
        "    \n",
        "    print(f\"Training on {num_samples} samples...\")\n",
        "    \n",
        "    # Callbacks\n",
        "    callbacks = [\n",
        "        keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=5,\n",
        "            restore_best_weights=True\n",
        "        ),\n",
        "        keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=3,\n",
        "            min_lr=1e-7\n",
        "        )\n",
        "    ]\n",
        "    \n",
        "    # Train model\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        epochs=epochs,\n",
        "        validation_data=val_gen,\n",
        "        callbacks=callbacks,\n",
        "        initial_epoch=initial_epoch,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    # Save model\n",
        "    metadata = {\n",
        "        'epochs_trained': epochs,\n",
        "        'total_samples': num_samples,\n",
        "        'last_accuracy': float(history.history['accuracy'][-1]),\n",
        "        'last_val_accuracy': float(history.history['val_accuracy'][-1])\n",
        "    }\n",
        "    \n",
        "    save_model(model, metadata=metadata)\n",
        "    \n",
        "    print(\"Model training completed and saved!\")\n",
        "    \n",
        "    return model, history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(model, audio_path):\n",
        "    \"\"\"\n",
        "    Make prediction on a single audio file.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained Keras model\n",
        "        audio_path: Path to audio file or file-like object\n",
        "    \n",
        "    Returns:\n",
        "        dict with 'prediction' (0 or 1), 'confidence' (float), 'probability' (float)\n",
        "    \"\"\"\n",
        "    from preprocessing import audio_file_to_image, image_to_array\n",
        "    \n",
        "    # Process audio\n",
        "    img = audio_file_to_image(audio_path)\n",
        "    img_array = image_to_array(img)\n",
        "    \n",
        "    # Add batch dimension\n",
        "    img_batch = np.expand_dims(img_array, axis=0)\n",
        "    \n",
        "    # Predict\n",
        "    prediction = model.predict(img_batch, verbose=0)[0][0]\n",
        "    \n",
        "    # Binary classification: 0 = Safe, 1 = Danger\n",
        "    class_idx = 1 if prediction > 0.5 else 0\n",
        "    confidence = abs(prediction - 0.5) * 2  # Convert to [0, 1] confidence\n",
        "    \n",
        "    return {\n",
        "        'prediction': int(class_idx),\n",
        "        'class': 'danger' if class_idx == 1 else 'safe',\n",
        "        'confidence': float(confidence),\n",
        "        'probability': float(prediction)  # Raw probability (0=Safe, 1=Danger)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interactive Training\n",
        "\n",
        "Now let's actually train the model with your data!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data directory: c:\\Users\\USER\\Documents\\Sentinel-End-to-End-MLOps\\backend\\data\n",
            "Safe directory exists: True\n",
            "Danger directory exists: True\n",
            "Safe audio files: 100\n",
            "Danger audio files: 100\n"
          ]
        }
      ],
      "source": [
        "# Check data directory structure\n",
        "from pathlib import Path\n",
        "\n",
        "data_dir = Path(\"data\")\n",
        "safe_dir = data_dir / \"safe\"\n",
        "danger_dir = data_dir / \"danger\"\n",
        "\n",
        "print(f\"Data directory: {data_dir.absolute()}\")\n",
        "print(f\"Safe directory exists: {safe_dir.exists()}\")\n",
        "print(f\"Danger directory exists: {danger_dir.exists()}\")\n",
        "\n",
        "if safe_dir.exists():\n",
        "    safe_files = list(safe_dir.glob(\"*.wav\"))\n",
        "    print(f\"Safe audio files: {len(safe_files)}\")\n",
        "    \n",
        "if danger_dir.exists():\n",
        "    danger_files = list(danger_dir.glob(\"*.wav\"))\n",
        "    print(f\"Danger audio files: {len(danger_files)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing model...\n",
            "Creating new model...\n",
            "New model created!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sentinel_mobilenet\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sentinel_mobilenet\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">163,968</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ predictions (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m163,968\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ predictions (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,430,273</span> (9.27 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,430,273\u001b[0m (9.27 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">172,289</span> (673.00 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m172,289\u001b[0m (673.00 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create or load model\n",
        "print(\"Initializing model...\")\n",
        "model = load_model()\n",
        "if model is None:\n",
        "    print(\"Creating new model...\")\n",
        "    model = create_model()\n",
        "    print(\"New model created!\")\n",
        "else:\n",
        "    print(\"Loaded existing model from disk\")\n",
        "    \n",
        "# Display model summary\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the Model\n",
        "\n",
        "Train the model on your preprocessed data. Adjust epochs, batch_size, and validation_split as needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Starting Model Training\n",
            "============================================================\n",
            "Epochs: 10\n",
            "Batch size: 32\n",
            "Validation split: 0.2\n",
            "Data directory: data\n",
            "============================================================\n",
            "Loading data from data...\n",
            "Training on 200 samples...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USER\\Documents\\Sentinel-End-to-End-MLOps\\venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - accuracy: 0.5437 - loss: 0.7590 - val_accuracy: 0.5750 - val_loss: 0.6784 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - accuracy: 0.6187 - loss: 0.6583 - val_accuracy: 0.7250 - val_loss: 0.5870 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5s/step - accuracy: 0.7188 - loss: 0.5988 - val_accuracy: 0.7000 - val_loss: 0.5743 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6s/step - accuracy: 0.6750 - loss: 0.6520 - val_accuracy: 0.7500 - val_loss: 0.5453 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6s/step - accuracy: 0.7437 - loss: 0.5248 - val_accuracy: 0.7250 - val_loss: 0.5771 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 6s/step - accuracy: 0.7812 - loss: 0.4869 - val_accuracy: 0.7750 - val_loss: 0.5274 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 5s/step - accuracy: 0.8000 - loss: 0.4609 - val_accuracy: 0.7500 - val_loss: 0.5392 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 6s/step - accuracy: 0.8250 - loss: 0.4954 - val_accuracy: 0.8250 - val_loss: 0.4774 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 5s/step - accuracy: 0.8125 - loss: 0.4548 - val_accuracy: 0.7500 - val_loss: 0.6083 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 5s/step - accuracy: 0.7875 - loss: 0.4061 - val_accuracy: 0.7750 - val_loss: 0.5666 - learning_rate: 0.0010\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model training completed and saved!\n",
            "\n",
            "Training completed!\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "# Adjust these parameters as needed:\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 32\n",
        "VALIDATION_SPLIT = 0.2\n",
        "DATA_DIR = \"data\"  # Should contain safe/ and danger/ subdirectories\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Starting Model Training\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Epochs: {EPOCHS}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Validation split: {VALIDATION_SPLIT}\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Train the model\n",
        "trained_model, training_history = train_model(\n",
        "    data_dir=DATA_DIR,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_split=VALIDATION_SPLIT,\n",
        "    model=model  # Use the model we created/loaded above\n",
        ")\n",
        "\n",
        "print(\"\\nTraining completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Training Results\n",
        "\n",
        "Plot training accuracy and loss over epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final Training Accuracy: 0.7875\n",
            "Final Validation Accuracy: 0.7750\n",
            "Final Training Loss: 0.4061\n",
            "Final Validation Loss: 0.5666\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2820\\3651325106.py:28: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
            "  plt.show()\n"
          ]
        }
      ],
      "source": [
        "# Plot training history\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history = training_history.history\n",
        "\n",
        "# Create figure with subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot accuracy\n",
        "ax1.plot(history['accuracy'], label='Training Accuracy', marker='o')\n",
        "ax1.plot(history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.set_title('Model Accuracy')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "# Plot loss\n",
        "ax2.plot(history['loss'], label='Training Loss', marker='o')\n",
        "ax2.plot(history['val_loss'], label='Validation Loss', marker='s')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.set_title('Model Loss')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print final metrics\n",
        "print(f\"\\nFinal Training Accuracy: {history['accuracy'][-1]:.4f}\")\n",
        "print(f\"Final Validation Accuracy: {history['val_accuracy'][-1]:.4f}\")\n",
        "print(f\"Final Training Loss: {history['loss'][-1]:.4f}\")\n",
        "print(f\"Final Validation Loss: {history['val_loss'][-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Predictions\n",
        "\n",
        "Test the trained model on sample audio files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing on SAFE file: yURIdR7A1oM_out.wav\n",
            "Prediction: SAFE\n",
            "Confidence: 54.76%\n",
            "Probability: 0.2262\n",
            "Expected: SAFE | Got: SAFE | ✓\n",
            "\n",
            "Testing on DANGER file: rDa7UMwSnBc_out.wav\n",
            "Prediction: SAFE\n",
            "Confidence: 20.54%\n",
            "Probability: 0.3973\n",
            "Expected: DANGER | Got: SAFE | ✗\n"
          ]
        }
      ],
      "source": [
        "# Test prediction on a sample file\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "# Get sample files\n",
        "data_dir = Path(\"data\")\n",
        "safe_files = list((data_dir / \"safe\").glob(\"*.wav\"))\n",
        "danger_files = list((data_dir / \"danger\").glob(\"*.wav\"))\n",
        "\n",
        "# Test on a random safe file\n",
        "if safe_files:\n",
        "    test_safe = random.choice(safe_files)\n",
        "    print(f\"Testing on SAFE file: {test_safe.name}\")\n",
        "    result = predict(trained_model, test_safe)\n",
        "    print(f\"Prediction: {result['class'].upper()}\")\n",
        "    print(f\"Confidence: {result['confidence']:.2%}\")\n",
        "    print(f\"Probability: {result['probability']:.4f}\")\n",
        "    print(f\"Expected: SAFE | Got: {result['class'].upper()} | {'✓' if result['class'] == 'safe' else '✗'}\")\n",
        "\n",
        "# Test on a random danger file\n",
        "if danger_files:\n",
        "    test_danger = random.choice(danger_files)\n",
        "    print(f\"\\nTesting on DANGER file: {test_danger.name}\")\n",
        "    result = predict(trained_model, test_danger)\n",
        "    print(f\"Prediction: {result['class'].upper()}\")\n",
        "    print(f\"Confidence: {result['confidence']:.2%}\")\n",
        "    print(f\"Probability: {result['probability']:.4f}\")\n",
        "    print(f\"Expected: DANGER | Got: {result['class'].upper()} | {'✓' if result['class'] == 'danger' else '✗'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Evaluation\n",
        "\n",
        "Evaluate the model on all validation data to get comprehensive metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing test data...\n",
            "\n",
            "============================================================\n",
            "Classification Report\n",
            "============================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Safe       0.71      0.85      0.77        20\n",
            "      Danger       0.81      0.65      0.72        20\n",
            "\n",
            "    accuracy                           0.75        40\n",
            "   macro avg       0.76      0.75      0.75        40\n",
            "weighted avg       0.76      0.75      0.75        40\n",
            "\n",
            "\n",
            "Confusion Matrix\n",
            "============================================================\n",
            "[[17  3]\n",
            " [ 7 13]]\n",
            "\n",
            "True Negatives (Safe→Safe): 17\n",
            "False Positives (Safe→Danger): 3\n",
            "False Negatives (Danger→Safe): 7\n",
            "True Positives (Danger→Danger): 13\n"
          ]
        }
      ],
      "source": [
        "# Evaluate model on test data\n",
        "from preprocessing import audio_file_to_image, image_to_array\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Prepare test data (you can adjust this to use a separate test set)\n",
        "test_data_dir = Path(\"data\")\n",
        "safe_test = list((test_data_dir / \"safe\").glob(\"*.wav\"))[:20]  # Test on subset\n",
        "danger_test = list((test_data_dir / \"danger\").glob(\"*.wav\"))[:20]\n",
        "\n",
        "X_test = []\n",
        "y_test = []\n",
        "\n",
        "print(\"Preparing test data...\")\n",
        "for file in safe_test:\n",
        "    try:\n",
        "        img = audio_file_to_image(file)\n",
        "        img_array = image_to_array(img)\n",
        "        X_test.append(img_array)\n",
        "        y_test.append(0)  # Safe\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file}: {e}\")\n",
        "\n",
        "for file in danger_test:\n",
        "    try:\n",
        "        img = audio_file_to_image(file)\n",
        "        img_array = image_to_array(img)\n",
        "        X_test.append(img_array)\n",
        "        y_test.append(1)  # Danger\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file}: {e}\")\n",
        "\n",
        "if X_test:\n",
        "    X_test = np.array(X_test)\n",
        "    y_test = np.array(y_test)\n",
        "    \n",
        "    # Make predictions\n",
        "    predictions = trained_model.predict(X_test, verbose=0)\n",
        "    y_pred = (predictions > 0.5).astype(int).flatten()\n",
        "    \n",
        "    # Print metrics\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Classification Report\")\n",
        "    print(\"=\" * 60)\n",
        "    print(classification_report(y_test, y_pred, target_names=['Safe', 'Danger']))\n",
        "    \n",
        "    print(\"\\nConfusion Matrix\")\n",
        "    print(\"=\" * 60)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(cm)\n",
        "    print(\"\\nTrue Negatives (Safe→Safe):\", cm[0][0])\n",
        "    print(\"False Positives (Safe→Danger):\", cm[0][1])\n",
        "    print(\"False Negatives (Danger→Safe):\", cm[1][0])\n",
        "    print(\"True Positives (Danger→Danger):\", cm[1][1])\n",
        "else:\n",
        "    print(\"No test data available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save and Use Model\n",
        "\n",
        "The model is automatically saved during training. You can also manually save it here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to: models\\sentinel_model.h5\n",
            "Metadata saved to: models\\model_metadata.json\n",
            "\n",
            "Verifying model can be loaded...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Model loaded successfully!\n",
            "Model parameters: 2,430,273\n"
          ]
        }
      ],
      "source": [
        "# The model is already saved by train_model(), but you can save again with custom metadata\n",
        "metadata = {\n",
        "    'training_completed': True,\n",
        "    'epochs': EPOCHS,\n",
        "    'final_accuracy': float(training_history.history['accuracy'][-1]),\n",
        "    'final_val_accuracy': float(training_history.history['val_accuracy'][-1]),\n",
        "}\n",
        "\n",
        "save_model(trained_model, metadata=metadata)\n",
        "print(f\"Model saved to: {MODEL_PATH}\")\n",
        "print(f\"Metadata saved to: {METADATA_PATH}\")\n",
        "\n",
        "# Verify model can be loaded\n",
        "print(\"\\nVerifying model can be loaded...\")\n",
        "loaded_model = load_model()\n",
        "if loaded_model:\n",
        "    print(\"✓ Model loaded successfully!\")\n",
        "    print(f\"Model parameters: {loaded_model.count_params():,}\")\n",
        "else:\n",
        "    print(\"✗ Failed to load model\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
